{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a BERT model\n",
    "\n",
    "### Import packages\n",
    "\n",
    "Import Python packages and display the Azure Machine Learning SDK version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1647873134302
    },
    "tags": [
     "check version"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  1.38.0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to workspace\n",
    "\n",
    "Create a workspace object from the existing workspace. `Workspace.from_config()` reads the file **config.json** and loads the details into an object named `ws`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1647873136039
    },
    "tags": [
     "load workspace"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs-ws\tfrancecentral\topenclassrooms\n"
     ]
    }
   ],
   "source": [
    "# load workspace configuration from the config.json file in the current folder.\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create experiment\n",
    "\n",
    "Create an experiment to track the runs in your workspace. A workspace can have muliple experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gather": {
     "logged": 1647873242195
    },
    "tags": [
     "create experiment"
    ]
   },
   "outputs": [],
   "source": [
    "experiment_name = 'bert_sentiment_analysis'\n",
    "\n",
    "from azureml.core import Experiment\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create or Attach existing compute resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1647873140276
    },
    "tags": [
     "create mlc",
     "amlcompute"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found compute target: cpu-cluster\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"cpu-cluster\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 0)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 4)\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print(\"found compute target: \" + compute_name)\n",
    "else:\n",
    "    print(\"creating new compute target...\")\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Explore data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1647873158134
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>polarity</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>wants to compete! i want hard competition! i w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>It seems we are stuck on the ground in Amarill...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>where the f are my pinking shears? rarararrrar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0ff t0 tHE MEEtiN..  i HAtE WhEN PPl V0lUNtEER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>@ reply me pls</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   polarity                                              tweet\n",
       "0         0  wants to compete! i want hard competition! i w...\n",
       "1         0  It seems we are stuck on the ground in Amarill...\n",
       "2         0  where the f are my pinking shears? rarararrrar...\n",
       "3         0  0ff t0 tHE MEEtiN..  i HAtE WhEN PPl V0lUNtEER...\n",
       "4         4                                    @ reply me pls "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "#TabularDataset\n",
    "datastore = ws.get_default_datastore()\n",
    "#1600tweets \n",
    "csv_path = [(datastore, 'UI/02-17-2022_021854_UTC/selected_tweets.csv')] \n",
    "dataset = Dataset.Tabular.from_delimited_files(path=csv_path)\n",
    "\n",
    "# load the TabularDataset to pandas DataFrame\n",
    "df_tweet = dataset.to_pandas_dataframe()\n",
    "\n",
    "print(df_tweet.shape)\n",
    "df_tweet.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display some sample images\n",
    "\n",
    "Load the compressed files into `numpy` arrays. Then use `matplotlib` to plot 30 random images from the dataset with their labels above them. Note this step requires a `load_data` function that's included in an `utils.py` file. This file is included in the sample folder. Please make sure it is placed in the same folder as this notebook. The `load_data` function simply parses the compresse files into numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on a remote cluster\n",
    "\n",
    "For this task, submit the job to run on the remote training cluster:\n",
    "* Create a directory\n",
    "* Save training and testing dataset to directory\n",
    "* Create a training script\n",
    "* Create a script run configuration\n",
    "* Submit the job \n",
    "\n",
    "### Create a directory\n",
    "\n",
    "Create a directory to deliver the necessary code from your computer to the remote resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gather": {
     "logged": 1647873159301
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "script_folder = os.path.join(os.getcwd(), \"bert_training\")\n",
    "os.makedirs(script_folder, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1647873163097
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df_tweet, test_size=0.1, random_state=0)\n",
    "\n",
    "#save to csv\n",
    "df_train.to_csv(script_folder+'//train.csv',index=False)\n",
    "df_test.to_csv(script_folder+'//test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training script\n",
    "\n",
    "To submit the job to the cluster, first create a training script. Run the following code to create the training script called `train.py` in the directory you just created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /mnt/batch/tasks/shared/LS_root/mounts/clusters/calcbert/code/Users/lei.xiaofan/sentiment/bert_training/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# Hide GPU from visible devices\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# let user feed in 2 parameters, the dataset to mount or download, and the regularization rate of the logistic regression model\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--train-file', type=str, dest='trainfile', help='Data sources')\n",
    "parser.add_argument('--test-file', type=str, dest='testfile', help='Data sources')\n",
    "parser.add_argument('--epochs', type=int, dest='e', default=1, help='epochs')\n",
    "args = parser.parse_args()\n",
    "\n",
    "####################################load the source file###################################\n",
    "df_train = pd.read_csv(args.trainfile, sep=',', encoding='UTF')\n",
    "df_test = pd.read_csv(args.testfile, sep=',', encoding='UTF')\n",
    "print(\"training size:\", len(df_train))\n",
    "print(\"testing size:\", len(df_test))\n",
    "\n",
    "X_train = df_train.tweet.values\n",
    "y_train = df_train.polarity.replace(4,1)\n",
    "X_test = df_test.tweet.values\n",
    "y_test = df_test.polarity.replace(4,1)\n",
    "\n",
    "########################Creating a vocabulary index and converting it to sequences#####################\n",
    "from transformers import BertTokenizer,TFBertForSequenceClassification\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def preprocess(X):\n",
    "    import re\n",
    "    def text_clean(text):\n",
    "        temp = text.lower()\n",
    "        temp = re.sub(\"@[A-Za-z0-9_]+\",\"\", temp)\n",
    "        temp = re.sub(\"#[A-Za-z0-9_]+\",\"\", temp)\n",
    "        temp = re.sub(r\"http\\S+\", \"\", temp)\n",
    "        temp = re.sub(r\"www.\\S+\", \"\", temp)\n",
    "        temp = re.sub(\"[0-9]\",\"\", temp)\n",
    "        return temp\n",
    "    X_cleaned = [text_clean(text) for text in X]\n",
    "    return X_cleaned\n",
    "\n",
    "#The encode_plus  function of the tokenizer class will tokenize the raw input, add the special tokens, and pad the vector \n",
    "def convert_example_to_feature(text):\n",
    "    return bert_tokenizer.encode_plus(text,\n",
    "            add_special_tokens = True, # add [CLS], [SEP]\n",
    "            max_length = 128, # max length of the text that can go to BERT\n",
    "            pad_to_max_length = True, # add [PAD] tokens\n",
    "            return_attention_mask = True, # add attention mask to not focus on pad tokens\n",
    "          )\n",
    "#he following helper functions will help us to transform our raw data to an appropriate format ready to feed into the BERT model\n",
    "def map_example_to_dict(input_ids, attention_masks, token_type_ids, label):\n",
    "    return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"token_type_ids\": token_type_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "    }, label\n",
    "def encode_examples(X,y):\n",
    "    # prepare list, so that we can build up final TensorFlow dataset from slices.\n",
    "    input_ids_list = []\n",
    "    token_type_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "    for text, label in zip(X, y):\n",
    "        bert_input = convert_example_to_feature(text)\n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        token_type_ids_list.append(bert_input['token_type_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "        label_list.append([label])\n",
    "    return tf.data.Dataset.from_tensor_slices((input_ids_list, attention_mask_list, token_type_ids_list, label_list)).map(map_example_to_dict)\n",
    "\n",
    "# train dataset\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "ds_train_encoded = encode_examples(preprocess(X_train), y_train).shuffle(100).batch(16).repeat(2)\n",
    "ds_val_encoded = encode_examples(preprocess(X_validation), y_validation).batch(16)\n",
    "# test dataset\n",
    "ds_test_encoded = encode_examples(preprocess(X_test), y_test).batch(16)\n",
    "########################training and evaluating the model###################################\n",
    "from azureml.core import Run\n",
    "\n",
    "# get hold of the current run\n",
    "run = Run.get_context()\n",
    "\n",
    "# recommended learning rate for Adam 5e-5, 3e-5, 2e-5\n",
    "learning_rate = 3e-5\n",
    "# model initialization\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "# choosing Adam optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08)\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "model.fit(ds_train_encoded, epochs=args.e, validation_data=ds_val_encoded)\n",
    "\n",
    "loss, acc = model.evaluate(ds_test_encoded, verbose=0)\n",
    "print(\"accuracy: {:5.2f}%\".format(100 * acc))\n",
    "\n",
    "run.log('epochs', args.e)\n",
    "run.log('accuracy', acc)\n",
    "\n",
    "#############save the model in two parts, one for keras, one for pipeline#####################\n",
    "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "# Save the model :\n",
    "model.save_pretrained(\"outputs/bert_model\", saved_model=True)\n",
    "\n",
    "if os.path.exists('outputs/bert_model') :\n",
    "    print(\"model saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the training job\n",
    "\n",
    "Configure the ScriptRunConfig by specifying:\n",
    "\n",
    "* The directory that contains your scripts. All the files in this directory are uploaded into the cluster nodes for execution. \n",
    "* The compute target.  In this case you will use the AmlCompute you created\n",
    "* The training script name, train.py\n",
    "* An environment that contains the libraries needed to run the script\n",
    "* Arguments required from the training script. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gather": {
     "logged": 1647873207029
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# to install required packages\\nenv = Environment('train-P7-bert')\\ncd = CondaDependencies.create(pip_packages=['azureml-dataset-runtime[pandas,fuse]','azureml-defaults','tensorflow==2.8.0', 'transformers==4.17.0','huggingface_hub' ],\\nconda_packages = ['pip','python==3.9.7','scikit-learn==1.0.2'])\\n\\nenv.python.conda_dependencies = cd\\n\\n# Register environment to re-use later\\nenv.register(workspace = ws)\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core.environment import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "env=Environment.get(workspace=ws, name='train-P7-bert', version=6)\n",
    "\"\"\"\n",
    "# to install required packages\n",
    "env = Environment('train-P7-bert')\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-dataset-runtime[pandas,fuse]','azureml-defaults','tensorflow==2.8.0', 'transformers==4.17.0','huggingface_hub' ],\n",
    "conda_packages = ['pip','python==3.9.7','scikit-learn==1.0.2'])\n",
    "\n",
    "env.python.conda_dependencies = cd\n",
    "\n",
    "# Register environment to re-use later\n",
    "env.register(workspace = ws)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, create the ScriptRunConfig by specifying the training script, compute target and environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "gather": {
     "logged": 1647876861321
    },
    "tags": [
     "configure estimator"
    ]
   },
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "\n",
    "args = ['--train-file', 'train.csv','--test-file', 'test.csv', '--epochs',1]\n",
    "\n",
    "src = ScriptRunConfig(source_directory=script_folder,\n",
    "                      script='train.py', \n",
    "                      arguments=args,\n",
    "                      compute_target=compute_target,\n",
    "                      environment=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the job to the cluster\n",
    "\n",
    "Run the experiment by submitting the ScriptRunConfig object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "gather": {
     "logged": 1647877652983
    },
    "tags": [
     "remote run",
     "amlcompute",
     "scikit-learn"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>bert_sentiment_analysis</td><td>bert_sentiment_analysis_1647877648_c5c49869</td><td>azureml.scriptrun</td><td>Starting</td><td><a href=\"https://ml.azure.com/runs/bert_sentiment_analysis_1647877648_c5c49869?wsid=/subscriptions/403c34e4-adde-4596-85b1-272a798d7ef2/resourcegroups/openclassrooms/workspaces/docs-ws&amp;tid=b7f12377-8b58-428a-84d4-37099d4cabbc\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: bert_sentiment_analysis,\n",
       "Id: bert_sentiment_analysis_1647877648_c5c49869,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Starting)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run = exp.submit(config=src)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the model in the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "gather": {
     "logged": 1647878805555
    },
    "tags": [
     "register model from history"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert model\tbert_model\tbert_model:2\t2\n"
     ]
    }
   ],
   "source": [
    "# register model \n",
    "model = run.register_model(model_name='bert_model', model_path='outputs/bert_model',\n",
    "                        tags={'Training context':'Script'},\n",
    "                    properties={'Accuracy': run.get_metrics()['accuracy']})\n",
    "print('bert model', model.name, model.id, model.version, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "gather": {
     "logged": 1647879366522
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'azureml-models/bert_model/1/bert_model'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_model_path(model_name='bert_model',version=1,_workspace=ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "gather": {
     "logged": 1647878773964
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(workspace=Workspace.create(name='docs-ws', subscription_id='403c34e4-adde-4596-85b1-272a798d7ef2', resource_group='openclassrooms'), name=bert_model, id=bert_model:1, version=1, tags={'Training context': 'Script'}, properties={'Accuracy': '0.78125'})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "Model(ws, 'bert_model',version=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Impressions](https://PixelServer20190423114238.azurewebsites.net/api/impressions/NotebookVM/tutorials/img-classification-part1-training.png)"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "maxluk"
   }
  ],
  "categories": [
   "tutorials",
   "image-classification-mnist-data"
  ],
  "kernel_info": {
   "name": "azureml_py38_pt_tf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "msauthor": "roastala",
  "network_required": false,
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
